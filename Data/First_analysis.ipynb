{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount gdrive support and set working directory for future use\n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "os.chdir('/content/drive/My Drive/emotion_detection')\n",
    "os.getcwd() # use this directory as root, all logs and model checkpoints are saved here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = pd.read_csv(\"{}/datasets/combined_data.csv\".format(os.getcwd()), sep='\\t')\n",
    "data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All the available datasets\n",
    "\n",
    "print(np.unique(data_df[\"dataset\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Description\n",
    "\n",
    "\n",
    "This data table contains texts from several datasets combined together. It contains  \n",
    "versatile variants text. The sources for texts include Reddit Posts,  \n",
    "tweets from different users, texts from dialgues coming from English textbooks  and manually collected sentences which were used for chat bot building. Merged datasets include  \n",
    "\n",
    "\n",
    "1.   [Empathetic Dialogues](https://github.com/facebookresearch/EmpatheticDialogues), [Paper](https://arxiv.org/pdf/1811.00207v5.pdf)\n",
    "2.   [CrowdFlower Tweet Data](https://www.kaggle.com/pashupatigupta/emotion-detection-from-text)\n",
    "3.   [GoEmotions Reddit Posts](https://github.com/google-research/google-research/tree/master/goemotions), [Paper](https://arxiv.org/pdf/2005.00547v2.pdf)\n",
    "4.   [DailyDialogue dialogue text](http://yanran.li/dailydialog), [Paper](https://arxiv.org/pdf/1710.03957.pdf)\n",
    "\n",
    "\n",
    "The versatility of texts that may help in build more robust emotion classifier. That is to say, a model can learn to discern emotional patterns both from contemporary style of social media texts and from something more formal, such as texts found in *EmpatheticDialogues* and *DailyDialogue* datasets.\n",
    "\n",
    "Texts coming from different datasets have several kinds of emotion labels. Some use very fine grained labels while some have labels only for small amount of emotions.  For instance, *EmpatheticDialogues* datasets uses 32 labels. To this end, we coalesce fine grained labels from the data into 6 basic emotions according  to Parrots emotion hierarchy ([Parrots Emotion Grouping](https://en.wikipedia.org/wiki/Emotion_classification)). A more visual explanation is given in the following wheel [Wheel of Emotions](https://www.becalmwithtati.com/wp-content/uploads/2019/02/Mindfulness_for_Emotions.jpeg).  \n",
    "\n",
    "This way, we obtain the *broadEmo* label in the dataset. Fine level emotion label is also present in the data so that both brad and fine emotions can be predicted with a model.  \n",
    "\n",
    "Moreover, the dataset name is also placed with each row so that dataset subsetting can be performed. This may be useful to train models only on specific kind of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data_df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nSentiment Counts:\")\n",
    "print(data_df.broadEmo.value_counts())\n",
    "print()\n",
    "classesBroad = data_df.broadEmo.unique().tolist()\n",
    "print(classesBroad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Huge imbalance towards neutral label\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(16, 8))\n",
    "\n",
    "ax.hist(data_df['broadEmo'])\n",
    "\n",
    "ax.set_xticklabels(list(classesBroad), rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nSentiment Counts:\")\n",
    "print(data_df.fineEmo.value_counts())\n",
    "print()\n",
    "classesFine = data_df.fineEmo.unique().tolist()\n",
    "print(classesFine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Huge imbalance towards neutral class. Consider using separate datasets from combination\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(16, 8))\n",
    "\n",
    "ax.hist(data_df['fineEmo'])\n",
    "\n",
    "ax.set_xticklabels(list(classesFine), rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find the max length here:\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"vinai/bertweet-base\", normalization=True)\n",
    "\n",
    "token_lens = []\n",
    "for txt in data_df[data_df.dataset == \"Empathetic Dialogues\"].sentence:\n",
    "  tokens = tokenizer.encode(txt)\n",
    "  token_lens.append(len(tokens))\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.hist(token_lens)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is too long, we should probably truncate our context + utterance to a certain threshold\n",
    "\n",
    "# Some of the sentences are too long. Luckily, not many of them are, we are safe to\n",
    "# Disregard those\n",
    "arr = np.array(token_lens)\n",
    "print(sum(arr[arr > 128]))\n",
    "max(token_lens)\n",
    "\n",
    "threshold_len = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "empDialogueData = data_df[data_df.dataset == \"Empathetic Dialogues\"]\n",
    "empDialogueData"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
