{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this to install latest version of PyTorch and Lightning\n",
    "\n",
    "!pip3 install pytorch-lightning\n",
    "!pip3 install torch\n",
    "!pip3 install transformers\n",
    "!pip3 install datasets\n",
    "!pip3 install emoji # to convert emoticons into their text versions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Model.EmoClassifier import TextClassifierModule\n",
    "from Data.Preprocessing import foo\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount gdrive support and set working directory for future use\n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "os.chdir('/content/drive/My Drive/emotion_detection')\n",
    "os.getcwd() # use this directory as root, all logs and model checkpoints are saved here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_data_path = \"{}/datasets/combined_data.csv\".format(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_module,tokenizer,classes = foo(  data_path=combined_data_path,\n",
    "                                label_name=\"broadEmo\",\n",
    "                                dataset_names=[\"Empathetic Dialogues\"], \n",
    "                                split_train_val_test=[0.8,0.1,0.1], \n",
    "                                max_len=60, \n",
    "                                tokenizer=\"BertweetTokenizer\", \n",
    "                                batch_size=32, \n",
    "                                RANDOM_SEED=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining network hpyeprapameters here. Hyperparameters can include anything and will be saved in model checkpoints\n",
    "# These hyperparameters will be avaialable in the neural network module.\n",
    "\n",
    "hparams = {\n",
    "  'freeze_feature_extractor': True,\n",
    "  'freeze_until_layer': 8, \n",
    "  'optimizer': 'Adam',\n",
    "  'featuer_extactor_lr': 3e-5,\n",
    "  'learning_rate': 3e-5,\n",
    "  'weight_decay': 1e-5,\n",
    "  'batch_size': 32, # needed in model for logging,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TextClassifierModule(len(classes), **hparams).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#summarize(model, max_depth=-1) # obtain full summary of the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorBoard Logging\n",
    "\n",
    "TensorBoard provides a good user interface to track training process.\n",
    "It can be used to track metric curves during traning and compare several versions of models at the same time. It also allows to view model architectures with input and output at each layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training\n",
    "\n",
    "Pytorch lightning Trainer takes in the data module and lightning module\n",
    "and performs the trainig loop automatically. It provides a bunch of useful\n",
    "callback to manage the training session (eg. early stopping, gradient tracking, weight averaging, etc.) Trainer can automatically track validation metrics and save best model checkpoints. Trainer removes the overhead of manually tracking every bit of training loop and eliminates boilerplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' \n",
    "Pytorch Lightning provides a Trainer class which handles training loop.\n",
    "The trainer automatically performs validation and training steps while also\n",
    "logging key metrics which enables effective training.\n",
    "\n",
    "The trainer takes in data modules and a model and automatically sets up the training loop\n",
    "\n",
    "The trainier automatically handles GPU or TPU training without the need of\n",
    "manually casting tensors to device.\n",
    "\n",
    "The trainer saves model checkpoints so that best models can later be recovered\n",
    "\n",
    "The trainer supports wide variety of options which make model training more efficient\n",
    "and fast (16 bit precision, debugging, early stopping, etc.)\n",
    "\n",
    "Full manual can be found here: https://pytorch-lightning.readthedocs.io/en/latest/common/trainer.html\n",
    "'''\n",
    "# Getting a warning about ambigous batch size\n",
    "# The warning is nothing serious, it happens because pytorch lightning\n",
    "# does not handle dictionaries as data inputs well. In reality training loop\n",
    "# works properly\n",
    "\n",
    "# Warning: https://github.com/PyTorchLightning/pytorch-lightning/issues/10349 \n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "classifier_logger = TensorBoardLogger(save_dir='lightning_logs', log_graph = True)\n",
    "\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "  monitor=\"val_loss\",\n",
    "  dirpath=\"models/\",\n",
    "  filename=\"emotion-recognizer\",\n",
    "  save_top_k=1,\n",
    "  mode=\"min\",\n",
    ")\n",
    "# !!TRAINING ON TPU causes exceptions for some reason. We have to stick to GPU for some time :(\n",
    "# It seem bertweet model is not compatible with TPU-s. This is stated in tensorbaord, under TPU compatability (0%)\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "  # overfit_batches=1, # debug option, overfits the given proportion of the whole data\n",
    "  track_grad_norm=2, # debug option, tracks gradient norms in tensorboard\n",
    "  default_root_dir=os.getcwd(), # The directory to save and log training results\n",
    "  max_epochs=30,\n",
    "  gpus=1 if torch.cuda.is_available() else None, # Uncomment to use GPU training when available\n",
    "  # tpu_cores=8, # Uncomment to train on TPU when it is available. Make sure to enable it before running with this flag\n",
    "  val_check_interval=0.1, # validate 10 times per epoch, frequent validation is helpful\n",
    "  logger=classifier_logger, # Logger options to track training\n",
    "  callbacks=[EarlyStopping(monitor=\"val_loss\", patience=10), checkpoint_callback],\n",
    "  # checkpoint_callback=False, # toggles checkpointing, might be good to avoid when debugging\n",
    "  # precision=16, # enable 16 bit precision. Allows multiple times faster training. Works only on GPU so far\n",
    ")\n",
    "\n",
    "trainer.fit(model, data_module)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
